{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhpL9e_MJat6"
   },
   "source": [
    "# Example source separation of a trumpet with Open-Unmix\n",
    "\n",
    "We provide an example of the source separation of a trumpet. The result shows that the model learns a meaningful task. However, the model complexity does not seem to be sufficiently complex to separate the trumpet from other wind instruments. This tasks is harder than for example separating voice from drum.\n",
    "We observe even less qualitative output for French horn or guitar because there sound is similar to other instruments in the considered ensembles. Moreover, they have longer pauses leading to a more imbalanced dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HwIoLlxnJQpa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'benedict' from 'benedict' (/home/nicolas/workspace/ma/ma4/ddspzart/.venv/lib/python3.8/site-packages/benedict/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/nicolas/workspace/ma/ma4/ddspzart/workspace_2/source_separation/1_source_separation_trumpet.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nicolas/workspace/ma/ma4/ddspzart/workspace_2/source_separation/1_source_separation_trumpet.ipynb#ch0000001?line=18'>19</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39moptim\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nicolas/workspace/ma/ma4/ddspzart/workspace_2/source_separation/1_source_separation_trumpet.ipynb#ch0000001?line=20'>21</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mopenunmix\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/nicolas/workspace/ma/ma4/ddspzart/workspace_2/source_separation/1_source_separation_trumpet.ipynb#ch0000001?line=22'>23</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdataloader_slakh\u001b[39;00m \u001b[39mimport\u001b[39;00m SlakhDataset\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nicolas/workspace/ma/ma4/ddspzart/workspace_2/source_separation/1_source_separation_trumpet.ipynb#ch0000001?line=24'>25</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39mrun_line_magic(\u001b[39m'\u001b[39m\u001b[39mload_ext\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mautoreload\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nicolas/workspace/ma/ma4/ddspzart/workspace_2/source_separation/1_source_separation_trumpet.ipynb#ch0000001?line=25'>26</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39mrun_line_magic(\u001b[39m'\u001b[39m\u001b[39mautoreload\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m2\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/workspace/ma/ma4/ddspzart/workspace_2/source_separation/dataloader_slakh.py:6\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmirdata\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbenedict\u001b[39;00m \u001b[39mimport\u001b[39;00m benedict\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlogging\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'benedict' from 'benedict' (/home/nicolas/workspace/ma/ma4/ddspzart/.venv/lib/python3.8/site-packages/benedict/__init__.py)"
     ]
    }
   ],
   "source": [
    "#import scipy.signal\n",
    "import pickle\n",
    "from dataloader_slakh import SlakhDataset\n",
    "import openunmix\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import norbert\n",
    "import sklearn.preprocessing\n",
    "import random\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from IPython.display import Audio, display\n",
    "print('Start')\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_KA4EvjOoH-"
   },
   "source": [
    "Initialize the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../open-unmix-pytorch/\n",
    "%pwd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8GFHW6kPfgW"
   },
   "source": [
    "## Set up the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = f\"../source_separation/data/checkpoints/\"\n",
    "CKPT_FILE = \"exp4_trumpet.pt\"\n",
    "DATASET_STAT_FILE = \"exp4_dataset_statistics.pickle\"\n",
    "%pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SAVE_PATH+DATASET_STAT_FILE, 'rb') as targets_file:\n",
    "    stat = pickle.load(targets_file)\n",
    "    mean, scale = stat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qCDQgoSlM3Ca"
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "device = \"cpu\"\n",
    "\n",
    "unmix = openunmix.model.OpenUnmix(\n",
    "    input_mean=mean,\n",
    "    input_scale=scale,\n",
    "    nb_channels=1,\n",
    "    hidden_size=512,\n",
    "    max_bin=512,\n",
    "    nb_bins=2048+1\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.RMSprop(unmix.parameters(), lr=0.005)\n",
    "criterion = torch.nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G32RgA0qRHsx"
   },
   "source": [
    "# Demonstrate the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing -> Here: Just use one sample.\n",
    "# We do not provide the whole data set here. For training, the whole SLAKH dataset was used.\n",
    "test_dataset = SlakhDataset(split='test', seq_duration=5.0)\n",
    "test_sampler = torch.utils.data.DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "track = test_dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the instrument (here Trumpet)\n",
    "test_dataset.target = 'Trumpet'\n",
    "test_dataset.filter_target()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model (here the trumpet source separation)\n",
    "checkpoint = torch.load(SAVE_PATH+CKPT_FILE)\n",
    "unmix.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate with custom trained models\n",
    "\n",
    "audio_torch = track[0][None, ...].float().to(device)\n",
    "\n",
    "# Here, we  only apply one source separation model to demonstrate that an application of openunmix is in general possible too for Wind instruments.\n",
    "# However, a more thorough neural network architecture optimization had to be performed for achieving better results.\n",
    "target_models = {\"Trumpet\": unmix}\n",
    "own_separator = openunmix.model.Separator(\n",
    "    target_models, nb_channels=1).to(device)\n",
    "y = track[1][None, ...].float().to(device).squeeze()\n",
    "\n",
    "y_hat = own_separator.forward(audio_torch).clone().detach().squeeze()\n",
    "\n",
    "display(Audio(track[0], rate=44100))\n",
    "display(Audio(y.cpu().numpy(), rate=44100))\n",
    "display(Audio(y_hat.cpu().numpy(), rate=44100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio2numpy import open_audio\n",
    "horn_extracted, sr = open_audio(\n",
    "    \"../source_separation/data/audio/01_horn_standard_not_augmented_extracted_05_loss.wav\")\n",
    "horn_isolated_gt, sr = open_audio(\n",
    "    \"../source_separation/data/audio/01_horn_standard_not_augmented_isolated.wav\")\n",
    "horn_with_others, sr = open_audio(\n",
    "    \"../source_separation/data/audio/01_horn_standard_not_augmented_original.wav\")\n",
    "\n",
    "print(\"Horn separated\")\n",
    "display(Audio(horn_with_others, rate=sr))\n",
    "display(Audio(horn_isolated_gt, rate=sr))\n",
    "display(Audio(horn_extracted, rate=sr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another example, we provide a French horn extraction. As mentioned, the model is not able to extract it. However, it can be heard that the melody voice (here trumpet) is removed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide testing functionality in the notebook \"evaluate_openunmix_separator.ipynb\". Here, we do not to provide a thorough discussion of the training process even though this needed time and effort also including the usage of the EPFL cluster.\n",
    "However, we think that this does not deliver key findings for the CM class.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of 2 - Training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f0c48a2510500372683021a8da20ae8a85be53b22e7c072f2fffd3068dfe896b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
